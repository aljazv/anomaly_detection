{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "import holoviews as hv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import param\n",
    "import paramnb\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from influxdb import DataFrameClient\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import stats\n",
    "from sklearn import metrics as metrics\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
    "                             fbeta_score, make_scorer, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import ricercando as ric\n",
    "\n",
    "database_ip = '46.101.250.119'\n",
    "ric.set_connection_params(host=database_ip)\n",
    "cli = DataFrameClient(database_ip, 8086, 'monroe', 'secure', 'monroe')\n",
    "cli.switch_database('monroe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page-Hinkley algorithm\n",
    "\n",
    "class PageHinkley:\n",
    "    def __init__(self, delta_=0.005, lambda_=50, alpha_=1 - 0.0001, detect_negative=False):\n",
    "        self.delta_ = delta_\n",
    "        self.lambda_ = lambda_\n",
    "        self.alpha_ = alpha_\n",
    "        self.sum = 0\n",
    "        # incrementally calculated mean of input data\n",
    "        self.x_mean = 0\n",
    "        # number of considered values\n",
    "        self.num = 0\n",
    "        self.change_detected = False\n",
    "        self.detect_negative = detect_negative\n",
    "\n",
    "    def __reset_params(self):\n",
    "        \"\"\"\n",
    "        Every time a change has been detected, all the collected statistics are reset.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.num = 0\n",
    "        self.x_mean = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def set_input(self, x):\n",
    "        \"\"\"\n",
    "        Main method for adding a new data value and automatically detect a possible concept drift.\n",
    "        :param x: input data\n",
    "        :return: boolean\n",
    "        \"\"\"\n",
    "        self.__detect_drift(x)\n",
    "        return self.change_detected\n",
    "\n",
    "    def __detect_drift(self, x):\n",
    "        \"\"\"\n",
    "        Concept drift detection following the formula from 'Knowledge Discovery from Data Streams' by JoÃ£o Gamma (p. 76)\n",
    "        :param x: input data\n",
    "        \"\"\"\n",
    "        # calculate the average and sum\n",
    "        self.num += 1\n",
    "        self.x_mean = (x + self.x_mean * (self.num - 1)) / self.num\n",
    "\n",
    "        if self.detect_negative:\n",
    "            self.sum = min(0.0, self.sum * self.alpha_ + (x - self.x_mean + self.delta_))\n",
    "        else:\n",
    "            self.sum = max(0.0, self.sum * self.alpha_ + (x - self.x_mean - self.delta_))\n",
    "\n",
    "        self.change_detected = True if abs(self.sum) > self.lambda_ else False\n",
    "        if self.change_detected:\n",
    "            self.__reset_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = [\n",
    "    {\n",
    "        \"node_id\": '601',\n",
    "        \"ICCID\": '89390100001965067610',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '608',\n",
    "        \"ICCID\": '8946071512360089522',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '609',\n",
    "        \"ICCID\": '89460850007007786482',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '610',\n",
    "        \"ICCID\": '8939104160000392272',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '612',\n",
    "        \"ICCID\": '8939104160000392231',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-29'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '613',\n",
    "        \"ICCID\": '89390100001965068626',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-29'\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom scoring\n",
    "\n",
    "# combining intervals\n",
    "def customise_score(y_testt, y_predd, offset = 5, mark_as=1):\n",
    "    \n",
    "    \n",
    "    y_t = np.copy(y_testt)\n",
    "    y_p = np.copy(y_predd)\n",
    "\n",
    "    \n",
    "    # Fill-in gaps betwwen test\n",
    "    for i in range(len(y_t)):\n",
    "        if y_t[i] and any(y_t[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_t[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_t[i+j] = mark_as\n",
    "        \n",
    "    # Fill-in gaps betwwen pred\n",
    "    for i in range(len(y_p)):\n",
    "        if y_p[i] and any(y_p[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_p[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_p[i+j] = mark_as\n",
    "                \n",
    "    return y_t, y_p\n",
    "\n",
    "# counting intervals\n",
    "\n",
    "def customise_score_for_readable(y_testt, y_predd, offset = 8, offset_pred = 8, mark_as = 1, mark_as_inverse = 0):\n",
    "    \n",
    "    \n",
    "    y_t = np.copy(y_testt)\n",
    "    y_p = np.copy(y_predd)\n",
    "\n",
    "\n",
    "    # Fill-in gaps between test marked True Classes\n",
    "    for i in range(len(y_t)):\n",
    "        if y_t[i] and any(y_t[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_t[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_t[i+j] = mark_as\n",
    "                    \n",
    "                    \n",
    "    # Fill-in gaps between pred marked True Classes\n",
    "    for i in range(len(y_p)):\n",
    "        if y_p[i] and any(y_p[i+1:i+offset_pred+1]):\n",
    "            for j in range(1,offset_pred+1):\n",
    "                if y_p[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_p[i+j] = mark_as\n",
    "                \n",
    "    return y_t, y_p\n",
    "            \n",
    "\n",
    "def customise_score_readable(*args, **kwargs):\n",
    "   \n",
    "    y_t, y_p = customise_score_for_readable(*args, **kwargs)\n",
    "    \n",
    "    if len(y_t) != len(y_p):\n",
    "        raise Exception(\"Invalid length od y_p and y_t, should be same\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    new_y_t = []\n",
    "    new_y_p = []\n",
    "    \n",
    "    \n",
    "    num_TN = 1\n",
    "    \n",
    "    # find TP and Fn\n",
    "    \n",
    "    while i < len(y_t):\n",
    "        if y_t[i]:\n",
    "            j = 1\n",
    "            while (i+j)<len(y_t) and y_t[i+j]:\n",
    "                j += 1\n",
    "            \n",
    "            if any(y_p[i:i+j]):\n",
    "                new_y_t.append(1)\n",
    "                new_y_p.append(1)\n",
    "                num_TN += 1\n",
    "                i = i + j\n",
    "                continue\n",
    "            else:\n",
    "                new_y_t.append(1)\n",
    "                new_y_p.append(0)\n",
    "                i = i + j\n",
    "                \n",
    "        i += 1\n",
    "    \n",
    "    # find TN - they dont matter- but number same as number of anomaly zones\n",
    "    \n",
    "    for i in range(num_TN):\n",
    "        new_y_t.append(0)\n",
    "        new_y_p.append(0)\n",
    "    \n",
    "    \n",
    "    # find FP\n",
    "                \n",
    "    while i < len(y_p):\n",
    "        if y_p[i]:\n",
    "            j = 1\n",
    "            while y_p[i+j]:\n",
    "                j += 1\n",
    "            \n",
    "            if not any(y_t[i:i+j]):\n",
    "                new_y_t.append(0)\n",
    "                new_y_p.append(1)\n",
    "                i = i + j\n",
    "                continue\n",
    "            else:\n",
    "                i = i + j\n",
    "                continue\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    return new_y_t, new_y_p\n",
    "\n",
    "\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    \n",
    "    #y_test_custom, y_pred_custom = customise_score(y_true ,y_pred,10)\n",
    "    y_test_custom, y_pred_custom = customise_score_readable(y_true, y_pred, offset = 10, offset_pred = 10)\n",
    "    precision = f1_score(y_true, y_pred) \n",
    "    \n",
    "    return precision\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input\n",
    "\n",
    "X_array = []\n",
    "X_train_array = []\n",
    "X_test_array = []\n",
    "\n",
    "y_array = []\n",
    "y_train_array = []\n",
    "y_test_array = []\n",
    "\n",
    "print_only_once = True\n",
    "\n",
    "columns_page = [\"Delta\",\"Lambda\",\"Alpha\"]\n",
    "page_hinkley_params = []\n",
    "\n",
    "for node in train_nodes:\n",
    "    node_id = node[\"node_id\"]\n",
    "    ICCID = node[\"ICCID\"]\n",
    "    start_time = node[\"start_time\"]\n",
    "    end_time = node[\"end_time\"]\n",
    "\n",
    "    datasets = cli.query(\"select * from class_1m where NodeId='{}' and time >= '{}' and time <= '{}' \".format(node_id,start_time,end_time))\n",
    "    df = ric.getdf(tables=\"ping\", nodeid=node_id,  start_time= start_time, end_time=end_time, freq=\"1m\")\n",
    "    df = df[df['Iccid'] == ICCID]\n",
    "\n",
    "    # merge together class and df\n",
    "    class_feature = datasets['class_1m'].copy()\n",
    "\n",
    "    class_feature = class_feature.drop(columns=['NodeId'])\n",
    "    class_feature.index = class_feature.index.tz_localize(None)\n",
    "    class_feature['time'] = class_feature.index\n",
    "    df['time'] = df.index\n",
    "    df.index.name = None\n",
    "    df = pd.merge(df, class_feature,  how='inner', left_on=['Iccid','time'], right_on = ['Iccid','time'])\n",
    "    df.index = df['time']\n",
    "    df = df.drop(columns=['time'])\n",
    "    df.index.name = 'time'\n",
    "    df_analise = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    # calculate page_hinkley parameters\n",
    "\n",
    "    seznam = dict()\n",
    "\n",
    "    adwin = []\n",
    "    page = []\n",
    "\n",
    "    df = df_analise.copy()\n",
    "\n",
    "    iccids = list(df[\"Iccid\"].unique())\n",
    "\n",
    "    df = df[df[\"Iccid\"] == ICCID]\n",
    "    \n",
    "    seznam = dict()\n",
    "\n",
    "    seznam[\"index\"] = []\n",
    "\n",
    "    seznam[\"razred\"] = []\n",
    "\n",
    "    seznam[\"page\"] = []\n",
    "    seznam[\"pageneg\"] = []\n",
    "    seznam[\"pageNames\"] = []\n",
    "\n",
    "    page_index = 0\n",
    "    print(\"calculating page for nodeID\",node_id)\n",
    "    for i in [0.005]:\n",
    "        for j in [80,100,150,200,300]:\n",
    "            for k in [0.01,0.05,0.03,0.08]:\n",
    "                seznam[\"page\"].append(PageHinkley(delta_=i, lambda_=j, alpha_=1 - k))\n",
    "                seznam[\"pageneg\"].append(PageHinkley(delta_=i, lambda_=j, alpha_=1 - k, detect_negative=True))\n",
    "                \n",
    "                if print_only_once:\n",
    "                    print(\"page_index {} delta: {} lambda: {} alpha: {}\".format(page_index,i,j,1-k))\n",
    "                    page_hinkley_params.append([i,j,k])\n",
    "                    page_index += 1\n",
    "                    \n",
    "\n",
    "    print_only_once = False\n",
    "    seznam[\"pageRes\"] = [[] for x in range(len(seznam[\"page\"]))]\n",
    "    seznam[\"pageResneg\"] = [[] for x in range(len(seznam[\"pageneg\"]))]\n",
    "\n",
    "    \n",
    "    print(len(df.index))\n",
    "    ppp = 0\n",
    "    for index, row in df.iterrows():\n",
    "        ppp += 1\n",
    "        if ppp % 1000 == 0:\n",
    "            print(ppp)\n",
    "        \n",
    "        iccid = row[\"Iccid\"]\n",
    "\n",
    "        if not math.isnan(row[\"RTT\"]):\n",
    "            seznam[\"index\"].append(index)\n",
    "\n",
    "        for i, page in enumerate(seznam[\"page\"]):\n",
    "\n",
    "            if not math.isnan(row[\"RTT\"]):\n",
    "                seznam[\"pageRes\"][i].append(page.set_input(row[\"RTT\"]))\n",
    "                seznam[\"pageResneg\"][i].append(page.set_input(row[\"RTT\"]))\n",
    "\n",
    "\n",
    "    i = 6\n",
    "    feature_list = []\n",
    "\n",
    "    for k, page in enumerate(seznam[\"pageRes\"]):\n",
    "        # or operation over two lists\n",
    "        new_list = [a or b for a,b in zip(page,seznam[\"pageRes\"][k])]\n",
    "        \n",
    "        Type_new = pd.Series(new_list,index=seznam[\"index\"])\n",
    "\n",
    "        df.insert(len(list(df.columns.values)), \"Page_\"+ str(k+1), Type_new)\n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    pima = df.copy()\n",
    "\n",
    "    #fill NaN with False\n",
    "    for k in range(1,len(seznam[\"pageRes\"])+1):\n",
    "        page_name = \"Page_{}\".format(k)\n",
    "        pima[page_name] = pima[page_name].fillna(False)\n",
    "\n",
    "    # add delay\n",
    "    print(\"adding delay\")\n",
    "    seznam_column = list(df.columns)\n",
    "    for index, row in pima.iterrows():\n",
    "        for column in seznam_column:\n",
    "            if \"Page\" in column and row[column]:\n",
    "                for i in range(1,4):\n",
    "                    time = index-datetime.timedelta(minutes=i)\n",
    "                    if time in pima.index:\n",
    "                        pima.at[index-datetime.timedelta(minutes=i), column] = True\n",
    "\n",
    "\n",
    "    for index in reversed(pima.index):\n",
    "        for column in seznam_column:\n",
    "            if \"Page\" in column and pima.loc[index, column]:\n",
    "\n",
    "                for i in range(1,4):\n",
    "                    time = index+datetime.timedelta(minutes=i)\n",
    "                    if time in pima.index:\n",
    "                        pima.at[index+datetime.timedelta(minutes=i), column] = True\n",
    "    print(\"done, pima is final df\")\n",
    "\n",
    "    features_names = list(filter(lambda x : \"Page_\" in x, seznam_column)) \n",
    "    y = pima['Class'].values\n",
    "    y = y * 1\n",
    "    X = pima[features_names].values\n",
    "\n",
    "    X_array.append(X)\n",
    "    y_array.append(y)\n",
    "    \n",
    "    \n",
    "X = np.concatenate(X_array)\n",
    "y = np.concatenate(y_array)\n",
    "\n",
    "DATA_SPLIT_PCT = 0.4\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "# evaluate Page-Hinkley parameters\n",
    "\n",
    "colummns = [\"TN\",\"FP\",\"FN\",\"TP\",\"Precision\",\"Recall\",\"F1\"]\n",
    "colummns = colummns + columns_page \n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_pred = X[:,i]\n",
    "    x_predd = x_pred * 1\n",
    "    yy = y * 1\n",
    "    \n",
    "    #y_testt, y_predd = customise_score(yy, x_predd,  offset = 8)\n",
    "    y_testt, y_predd = yy, x_predd\n",
    "    pre = precision_score(y_testt,y_predd)\n",
    "    recall = recall_score(y_testt,y_predd)\n",
    "    f1 = f1_score(y_testt,y_predd)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_testt,y_predd)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_testt, y_predd).ravel()\n",
    "    \n",
    "    row = [i,tn,fp,fn,tp,pre,recall,f1]\n",
    "    row = row + page_hinkley_params[i]\n",
    "    res.append(row)\n",
    "\n",
    "res = np.array(res)\n",
    "df_page = pd.DataFrame(data=res[:,1:],index=res[:,0],columns=colummns)\n",
    "df_page.sort_values(by=[\"F1\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# hyper parameterization\n",
    "\n",
    "scores = [\"my_custom_score\"]\n",
    "LABELS = [\"False\",\"True\"]\n",
    "\n",
    "names = [\"Linear SVM\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"Naive Bayes\"]\n",
    "\n",
    "my_cv = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "grid_params = [\n",
    "    {\n",
    "        'C': [0.01,0.1,1,10]\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'max_depth': [4,8,16,None],\n",
    "        'min_samples_split': np.linspace(0.1, 0.6, 4, endpoint=True),\n",
    "        'min_samples_leaf' : [1,2,4],\n",
    "        'max_features': ['auto','log2',None]\n",
    "        \n",
    "    },\n",
    "    \n",
    "     {\n",
    "    'n_estimators': [16,32,64,256],\n",
    "    'criterion': ['gini'],\n",
    "    \"max_features\" : ['auto','log2',None],\n",
    "    \"max_depth\" : [None,2,4]\n",
    "    },\n",
    "    \n",
    "     {\n",
    "    'n_estimators': [32,64,128,256],\n",
    "    'learning_rate':[0.1,1]\n",
    "    },\n",
    "    \n",
    "     {\n",
    "    },\n",
    "        \n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(max_depth=100),\n",
    "    RandomForestClassifier(max_depth=20, n_estimators=20, max_features=\"auto\"),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "for score in scores:\n",
    "    node_table = []\n",
    "    i = 0\n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        row = dict()\n",
    "        row[\"name\"] = names[i]\n",
    "        \n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "        print(\"Classifier: \", names[i])\n",
    "\n",
    "        tuned_parameters=grid_params[i]\n",
    "        i += 1\n",
    "\n",
    "        clf = GridSearchCV(classifier, tuned_parameters, cv=my_cv,\n",
    "                           scoring=my_custom_score, n_jobs=1)\n",
    "        models.append(clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        \n",
    "        classifier_cv = []\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            \n",
    "            row_param = params.copy()\n",
    "            row_param[\"mean\"] = mean\n",
    "            row_param[\"std\"] = std\n",
    "            classifier_cv.append(row_param)\n",
    "            \n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "        \n",
    "        row[\"best_params\"] = clf.best_params_\n",
    "        row[\"cv_params\"] = classifier_cv\n",
    "        \n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        row[\"f1_score\"] = metrics.f1_score(y_true,y_pred)\n",
    "        row[\"accuracy\"] = metrics.accuracy_score(y_true, y_pred)\n",
    "        row[\"precision\"] = metrics.precision_score(y_true, y_pred)\n",
    "        row[\"recall\"] = metrics.recall_score(y_true, y_pred)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        row[\"tn\"] = tn\n",
    "        row[\"fp\"] = fp\n",
    "        row[\"fn\"] = fn\n",
    "        row[\"tp\"] = tp\n",
    "        \n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "        plt.title(\"Confusion matrix\")\n",
    "        plt.ylabel('True class')\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "        # fit_predict\n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        row[\"auc\"] = roc_auc\n",
    "\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "        # fit predict prob\n",
    "\n",
    "        '''fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()'''\n",
    "\n",
    "         # CUSTOM\n",
    "\n",
    "        y_custom_test, y_custom_pred = customise_score(y_test,y_pred,10)\n",
    "\n",
    "        print(\"Accuracy for custom test: \",metrics.accuracy_score(y_custom_test, y_custom_pred))\n",
    "        print(classification_report(y_custom_test, y_custom_pred))\n",
    "        conf_matrix = confusion_matrix(y_custom_test,y_custom_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        row[\"tn_ad\"] = tn\n",
    "        row[\"fp_ad\"] = fp\n",
    "        row[\"fn_ad\"] = fn\n",
    "        row[\"tp_ad\"] = tp\n",
    "        \n",
    "        # row[\"classification_report_ad\"] = classification_report(y_custom_test, y_custom_pred, output_dict =True)\n",
    "        \n",
    "        row[\"f1_score_ad\"] = metrics.f1_score(y_custom_test,y_custom_pred)\n",
    "        row[\"accuracy_ad\"] = metrics.accuracy_score(y_custom_test, y_custom_pred)\n",
    "        row[\"precision_ad\"] = metrics.precision_score(y_custom_test, y_custom_pred)\n",
    "        row[\"recall_ad\"] = metrics.recall_score(y_custom_test, y_custom_pred)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "        plt.title(\"Confusion matrix for adjusted results\")\n",
    "        plt.ylabel('True class')\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y_custom_test, y_custom_pred)\n",
    "\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        row[\"auc_ad\"] = roc_auc\n",
    "\n",
    "        plt.title('Receiver Operating Characteristic for adjusted results')\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.show()\n",
    "        \n",
    "        node_table.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CV with best parameters\n",
    "\n",
    "LABELS = [\"False\",\"True\"]\n",
    "\n",
    "names = [ \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear SVM\"]\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    \n",
    "    DecisionTreeClassifier(max_depth=None,max_features='log2',min_samples_leaf=4,min_samples_split=0.1),\n",
    "    RandomForestClassifier(criterion='gini', max_depth=None, n_estimators=32, max_features=\"log2\"),\n",
    "    AdaBoostClassifier(learning_rate=1, n_estimators=32),\n",
    "    GaussianNB(),\n",
    "    LinearSVC(C=1),\n",
    "    ]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# fit model\n",
    "kfold = KFold(n_splits = 5, shuffle=False)\n",
    "\n",
    "for logreg in classifiers:\n",
    "    vse = []\n",
    "    print(\"Classifier: \", names[i])\n",
    "    for train, test in kfold.split(X, y):\n",
    "        row = dict()\n",
    "        #print('train: %s, test: %s' % (len(X[train]), len(X[test])))\n",
    "        \n",
    "        logreg.fit(X[train], y[train])\n",
    "        \n",
    "        y_test = y[test]\n",
    "        y_true = y_test\n",
    "        \n",
    "        y_pred = logreg.predict(X[test])\n",
    "        \n",
    "        # print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n",
    "        row[\"f1_score\"] = metrics.f1_score(y_true,y_pred)\n",
    "        row[\"accuracy\"] = metrics.accuracy_score(y_true, y_pred)\n",
    "        row[\"precision\"] = metrics.precision_score(y_true, y_pred)\n",
    "        row[\"recall\"] = metrics.recall_score(y_true, y_pred)\n",
    "        \n",
    "        \n",
    "        y_true, y_pred = customise_score(y_test,y_pred,10)\n",
    "        \n",
    "        row[\"f1_score_1\"] = metrics.f1_score(y_true,y_pred)\n",
    "        row[\"accuracy_1\"] = metrics.accuracy_score(y_true, y_pred)\n",
    "        row[\"precision_1\"] = metrics.precision_score(y_true, y_pred)\n",
    "        row[\"recall_1\"] = metrics.recall_score(y_true, y_pred)\n",
    "        \n",
    "        y_true, y_pred = customise_score_readable(y_true, y_pred, offset = 10, offset_pred = 10)\n",
    "        \n",
    "        row[\"f1_score_2\"] = metrics.f1_score(y_true,y_pred)\n",
    "        row[\"accuracy_2\"] = metrics.accuracy_score(y_true, y_pred)\n",
    "        row[\"precision_2\"] = metrics.precision_score(y_true, y_pred)\n",
    "        row[\"recall_2\"] = metrics.recall_score(y_true, y_pred)\n",
    "        \n",
    "        vse.append(row)\n",
    "        \n",
    "    v = {k: [dic[k] for dic in vse] for k in vse[0]}\n",
    "    print(v)\n",
    "    \n",
    "    for key in v:\n",
    "        print(f\"{key}: %0.2f (+/- %0.2f)\" % (np.array(v[key]).mean(), np.array(v[key]).std() ))\n",
    "    \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
