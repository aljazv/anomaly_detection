{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import math\n",
    "import os\n",
    "import talos as ta\n",
    "\n",
    "import holoviews as hv\n",
    "import keras_metrics as km\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from influxdb import DataFrameClient\n",
    "from keras import Sequential\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import (LSTM, BatchNormalization, Dense, Dropout, Flatten,\n",
    "                          Input, RepeatVector, TimeDistributed)\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate, concatenate\n",
    "from keras.models import Model\n",
    "from numpy.random import seed\n",
    "\n",
    "\n",
    "from pylab import rcParams\n",
    "from scipy import stats\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import (auc, classification_report, confusion_matrix,\n",
    "                             f1_score, precision_recall_curve,\n",
    "                             precision_recall_fscore_support, recall_score,\n",
    "                             roc_curve)\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "\n",
    "import ricercando as ric\n",
    "\n",
    "database_ip = '46.101.250.119'\n",
    "ric.set_connection_params(host=database_ip)\n",
    "cli = DataFrameClient(database_ip, 8086, 'monroe', 'secure', 'monroe')\n",
    "cli.switch_database('monroe')\n",
    "\n",
    "seed(7)\n",
    "set_random_seed(11)\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "LABELS = [\"False\",\"True\"]\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "DATA_SPLIT_PCT_VALID = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select nodes\n",
    "train_nodes = [\n",
    "    {\n",
    "        \"node_id\": '601',\n",
    "        \"ICCID\": '89390100001965067610',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '608',\n",
    "        \"ICCID\": '8946071512360089522',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '609',\n",
    "        \"ICCID\": '89460850007007786482',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '610',\n",
    "        \"ICCID\": '8939104160000392272',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-30'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '612',\n",
    "        \"ICCID\": '8939104160000392231',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-29'\n",
    "    },\n",
    "    {\n",
    "        \"node_id\": '613',\n",
    "        \"ICCID\": '89390100001965068626',\n",
    "        \"start_time\": '2018-01-01',\n",
    "        \"end_time\": '2018-01-29'\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [1,5,10,20,40]\n",
    "lookback = 240\n",
    "n_features = 1\n",
    "\n",
    "X_train_scaled_windows= []\n",
    "X_valid_scaled_windows= []\n",
    "X_test_scaled_windows= []\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    \n",
    "\n",
    "    X_array = []\n",
    "    X_train_array = []\n",
    "    X_valid_array = []\n",
    "    X_test_array = []\n",
    "\n",
    "    y_array = []\n",
    "    y_train_array = []\n",
    "    y_valid_array = []\n",
    "    y_test_array = []\n",
    "\n",
    "    for node in train_nodes:\n",
    "        node_id = node[\"node_id\"]\n",
    "        ICCID = node[\"ICCID\"]\n",
    "        start_time = node[\"start_time\"]\n",
    "        end_time = node[\"end_time\"]\n",
    "\n",
    "        datasets = cli.query(\"select * from class_1m where NodeId='{}' and time >= '{}' and time <= '{}' \".format(node_id,start_time,end_time))\n",
    "        df = ric.getdf(tables=\"ping\", nodeid=node_id,  start_time= start_time, end_time=end_time, freq=\"1m\")\n",
    "        df = df[df['Iccid'] == ICCID]\n",
    "\n",
    "        # merge together class and df\n",
    "        class_feature = datasets['class_1m'].copy()\n",
    "\n",
    "        class_feature = class_feature.drop(columns=['NodeId'])\n",
    "        class_feature.index = class_feature.index.tz_localize(None)\n",
    "        class_feature['time'] = class_feature.index\n",
    "        df['time'] = df.index\n",
    "        df.index.name = None\n",
    "        df = pd.merge(df, class_feature,  how='inner', left_on=['Iccid','time'], right_on = ['Iccid','time'])\n",
    "        df.index = df['time']\n",
    "        df = df.drop(columns=['time'])\n",
    "        df.index.name = 'time'\n",
    "        df_analise = df.copy()\n",
    "\n",
    "        # delay it for lookback value and predict from last element\n",
    "\n",
    "        df = df_analise.copy()\n",
    "        df = df.dropna(subset=['RTT'])\n",
    "        df.index = list(range(len(df.index)))\n",
    "        df = df[[\"RTT\",\"Class\"]]\n",
    "        df['Class'] = df['Class'].values * 1\n",
    "    \n",
    "\n",
    "        df = df.fillna(0)\n",
    "        df['RTT'] = df['RTT'].rolling(window_size, min_periods=1).mean()\n",
    "\n",
    "\n",
    "        first_RTT = df['RTT'][0]\n",
    "        last_RTT = df['RTT'].values[-1]\n",
    "\n",
    "\n",
    "        for i in range((int(lookback/2))-1,-1,-1):\n",
    "            df['RTT_-{}'.format(i)] = df['RTT'].shift(periods=i).fillna(first_RTT)\n",
    "\n",
    "        for i in range(1,(int(lookback/2)+1),1):\n",
    "            df['RTT_{}'.format(i)] = df['RTT'].shift(periods=-i).fillna(last_RTT)\n",
    "\n",
    "        columns_list = list(df.columns.values)\n",
    "        features_names = list(filter(lambda x : \"RTT_\" in x, columns_list))\n",
    "\n",
    "\n",
    "        X = df[features_names].values\n",
    "        y = df[\"Class\"].values \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=DATA_SPLIT_PCT, shuffle=False)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=DATA_SPLIT_PCT_VALID, shuffle=False)\n",
    "\n",
    "        X_train_array.append(X_train)\n",
    "        X_valid_array.append(X_valid)\n",
    "        X_test_array.append(X_test)\n",
    "\n",
    "        y_train_array.append(y_train)\n",
    "        y_valid_array.append(y_valid)\n",
    "        y_test_array.append(y_test)\n",
    "\n",
    "\n",
    "    X_train = np.concatenate(X_train_array)\n",
    "    X_valid = np.concatenate(X_valid_array)\n",
    "    X_test = np.concatenate(X_test_array)\n",
    "\n",
    "\n",
    "    y_train = np.concatenate(y_train_array)\n",
    "    y_valid = np.concatenate(y_valid_array)\n",
    "    y_test = np.concatenate(y_test_array)\n",
    "\n",
    "    sc = MinMaxScaler()\n",
    "    X_train_scaled = sc.fit_transform(X_train)\n",
    "    X_valid_scaled = sc.transform(X_valid)\n",
    "    X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train.shape[0], lookback, n_features)\n",
    "    X_valid_scaled = X_valid_scaled.reshape(X_valid.shape[0], lookback, n_features)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape[0], lookback, n_features)\n",
    "\n",
    "    X_train_scaled_windows.append(X_train_scaled)\n",
    "    X_valid_scaled_windows.append(X_valid_scaled)\n",
    "    X_test_scaled_windows.append(X_test_scaled)\n",
    "    \n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom scoring\n",
    "\n",
    "# combining intervals\n",
    "def customise_score(y_testt, y_predd, offset = 5, mark_as=1):\n",
    "    \n",
    "    \n",
    "    y_t = np.copy(y_testt)\n",
    "    y_p = np.copy(y_predd)\n",
    "\n",
    "    \n",
    "    # Fill-in gaps betwwen test\n",
    "    for i in range(len(y_t)):\n",
    "        if y_t[i] and any(y_t[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_t[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_t[i+j] = mark_as\n",
    "        \n",
    "    # Fill-in gaps betwwen pred\n",
    "    for i in range(len(y_p)):\n",
    "        if y_p[i] and any(y_p[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_p[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_p[i+j] = mark_as\n",
    "                \n",
    "    return y_t, y_p\n",
    "\n",
    "# counting intervals\n",
    "\n",
    "def customise_score_for_readable(y_testt, y_predd, offset = 8, offset_pred = 8, mark_as = 1, mark_as_inverse = 0):\n",
    "    \n",
    "    \n",
    "    y_t = np.copy(y_testt)\n",
    "    y_p = np.copy(y_predd)\n",
    "\n",
    "\n",
    "    # Fill-in gaps between test marked True Classes\n",
    "    for i in range(len(y_t)):\n",
    "        if y_t[i] and any(y_t[i+1:i+offset+1]):\n",
    "            for j in range(1,offset+1):\n",
    "                if y_t[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_t[i+j] = mark_as\n",
    "                    \n",
    "                    \n",
    "    # Fill-in gaps between pred marked True Classes\n",
    "    for i in range(len(y_p)):\n",
    "        if y_p[i] and any(y_p[i+1:i+offset_pred+1]):\n",
    "            for j in range(1,offset_pred+1):\n",
    "                if y_p[i+j]:\n",
    "                    break\n",
    "                else:\n",
    "                    y_p[i+j] = mark_as\n",
    "                \n",
    "    return y_t, y_p\n",
    "            \n",
    "\n",
    "def customise_score_readable(*args, **kwargs):\n",
    "   \n",
    "    y_t, y_p = customise_score_for_readable(*args, **kwargs)\n",
    "    \n",
    "    if len(y_t) != len(y_p):\n",
    "        raise Exception(\"Invalid length od y_p and y_t, should be same\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    new_y_t = []\n",
    "    new_y_p = []\n",
    "    \n",
    "    \n",
    "    num_TN = 1\n",
    "    \n",
    "    # find TP and Fn\n",
    "    \n",
    "    while i < len(y_t):\n",
    "        if y_t[i]:\n",
    "            j = 1\n",
    "            while y_t[i+j]:\n",
    "                j += 1\n",
    "            \n",
    "            if any(y_p[i:i+j]):\n",
    "                new_y_t.append(1)\n",
    "                new_y_p.append(1)\n",
    "                num_TN += 1\n",
    "                i = i + j\n",
    "                continue\n",
    "            else:\n",
    "                new_y_t.append(1)\n",
    "                new_y_p.append(0)\n",
    "                i = i + j\n",
    "                \n",
    "        i += 1\n",
    "    \n",
    "    # find TN - they dont matter- but number same as number of anomaly zones\n",
    "    \n",
    "    for i in range(num_TN):\n",
    "        new_y_t.append(0)\n",
    "        new_y_p.append(0)\n",
    "    \n",
    "    \n",
    "    # find FP\n",
    "                \n",
    "    while i < len(y_p):\n",
    "        if y_p[i]:\n",
    "            j = 1\n",
    "            while y_p[i+j]:\n",
    "                j += 1\n",
    "            \n",
    "            if not any(y_t[i:i+j]):\n",
    "                new_y_t.append(0)\n",
    "                new_y_p.append(1)\n",
    "                i = i + j\n",
    "                continue\n",
    "            else:\n",
    "                i = i + j\n",
    "                continue\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    return new_y_t, new_y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "def build_keras_base(x_train, y_train, x_val, y_val, params):\n",
    "    paramss = params\n",
    "    lr = 0.0001\n",
    "\n",
    "    input_branches = []\n",
    "    output_branches = []\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        visible = Input(shape=(240,1))\n",
    "        conv1 = Conv1D(filters=paramss['num_of_filters1'], kernel_size=paramss['kernel1'], activation='relu')(visible)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "        \n",
    "\n",
    "        input_branches.append(visible)\n",
    "        output_branches.append(pool1)\n",
    "\n",
    "\n",
    "    merge = concatenate(output_branches)\n",
    "    \n",
    "    conv2 = Conv1D(filters=paramss['num_of_filters2'], kernel_size=paramss['kernel2'], activation='relu')(merge)\n",
    "    pool2 = MaxPooling1D(pool_size=paramss['pool1'])(conv2)\n",
    "    flat = Flatten()(pool2)\n",
    "    \n",
    "    hidden1 = Dense(paramss['num_of_dense1'], activation='relu')(flat)\n",
    "    \n",
    "    if paramss['is_dropout']:\n",
    "        dropout1 = Dropout(0.4)(hidden1)\n",
    "    else:\n",
    "        dropout1 = BatchNormalization()(hidden1)\n",
    "        \n",
    "    model = None\n",
    "    if paramss['second_dense']:\n",
    "        \n",
    "        hidden2 = Dense(paramss['second_dense'], activation='relu')(dropout1)\n",
    "        \n",
    "        if paramss['is_dropout']:\n",
    "            dropout2 = Dropout(0.4)(hidden2)\n",
    "        else:\n",
    "            dropout2 = BatchNormalization()(hidden2)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        model = Model(inputs=input_branches, outputs=output)\n",
    "        \n",
    "        adam = optimizers.Adam(lr)\n",
    "        model.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy',km.precision(), km.recall()])\n",
    "        \n",
    "    else:\n",
    "        output = Dense(1, activation='sigmoid')(dropout1)\n",
    "        model = Model(inputs=input_branches, outputs=output)\n",
    "        adam = optimizers.Adam(lr)\n",
    "        model.compile(optimizer=adam, loss='binary_crossentropy',metrics=['accuracy',km.precision(), km.recall()])\n",
    "        \n",
    "        \n",
    "    es = EarlyStopping(monitor='val_loss', patience=4, verbose=0)\n",
    "    history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, validation_data=(x_val,y_val), class_weight=paramss['class_weights'], callbacks= [es])\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "p = {'num_of_dense1': [128, 256, 512],\n",
    "     'num_of_filters1':[8,16,32],\n",
    "     'num_of_filters2':[64,128,256],\n",
    "     'kernel1': [3,5,9],\n",
    "     'kernel2': [3,5,9],\n",
    "     'second_dense': [32,64, 128],\n",
    "     'is_dropout':[True],\n",
    "     'class_weights': [class_weights],\n",
    "     'pool1': [2,4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan random parameters\n",
    "\n",
    "t = ta.Scan(x=X_train_scaled_windows,\n",
    "            y=y_train,\n",
    "            x_val=X_valid_scaled_windows,\n",
    "            y_val=y_valid,\n",
    "            model=build_keras_base, \n",
    "            params=p,\n",
    "            experiment_name='hyperparam1',\n",
    "           fraction_limit=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze results\n",
    "analyze_object = ta.Analyze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the dataframe with the results\n",
    "\n",
    "# analyze_object.data['val_f1'] = (2* prec * recall) / (prec + recall)\n",
    "analyze_object.data.sort_values(by=['val_f1'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the highest result for any metric\n",
    "analyze_object.high('val_precision')\n",
    "\n",
    "# get the round with the best result\n",
    "analyze_object.rounds2high('val_acc')\n",
    "\n",
    "# get the best paramaters\n",
    "analyze_object.best_params('val_acc', ['acc', 'loss', 'val_loss'])\n",
    "\n",
    "# get correlation for hyperparameters against a metric\n",
    "analyze_object.correlate('val_loss', ['acc', 'loss', 'val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
